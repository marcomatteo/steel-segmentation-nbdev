{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# missing\n",
    "!git clone https://github.com/marcomatteo/steel_segmentation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# missing\n",
    "!pip install -e steel_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> A collection of Metrics used in the segmentation models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcomatteo/steel_segmentation/blob/master/dev_nbs/05_metrics.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from steel_segmentation.metadata import *\n",
    "from steel_segmentation.masks import *\n",
    "from steel_segmentation.datasets import *\n",
    "from steel_segmentation.dataloaders import *\n",
    "\n",
    "import fastai\n",
    "from fastai.vision.all import *\n",
    "from fastcore.foundation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics for fastai API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code from the fastai docs to test properly the metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing: a fake learner and a metric that isn't an average\n",
    "@delegates()\n",
    "class TstLearner(Learner):\n",
    "    def __init__(self,dls=None,model=None,**kwargs): self.pred,self.xb,self.yb = None,None,None\n",
    "        \n",
    "#Go through a fake cycle with various batch sizes and computes the value of met\n",
    "def compute_val(met, x1, x2):\n",
    "    met.reset()\n",
    "    vals = [0,6,15,20]\n",
    "    learn = TstLearner()\n",
    "    for i in range(3):\n",
    "        learn.pred,learn.yb = x1[vals[i]:vals[i+1]], (x2[vals[i]:vals[i+1]],)\n",
    "        met.accumulate(learn)\n",
    "    return met.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ModDiceMulti(Metric):\n",
    "    \"Averaged Dice metric (Macro F1) for multiclass target in segmentation\"\n",
    "\n",
    "    def __init__(self, axis=1): self.axis = axis\n",
    "    def reset(self): self.inter, self.union = {}, {}\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        pred = learn.pred.argmax(dim=self.axis)\n",
    "        y = learn.yb[0]\n",
    "        \n",
    "        if pred.shape != y.shape:\n",
    "            y = y.argmax(dim=self.axis)\n",
    "            \n",
    "        pred, targ = flatten_check(pred, y)\n",
    "        for c in range(learn.pred.shape[self.axis]):\n",
    "            p = torch.where(pred == c, 1, 0)\n",
    "            t = torch.where(targ == c, 1, 0)\n",
    "            p, t = TensorBase(p), TensorBase(t)\n",
    "            c_inter = (p*t).float().sum().item()\n",
    "            c_union = (p+t).float().sum().item()\n",
    "            if c in self.inter:\n",
    "                self.inter[c] += c_inter\n",
    "                self.union[c] += c_union\n",
    "            else:\n",
    "                self.inter[c] = c_inter\n",
    "                self.union[c] = c_union\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        binary_dice_scores = np.array([])\n",
    "        for c in self.inter:\n",
    "            binary_dice_scores = np.append(\n",
    "                binary_dice_scores, 2.*self.inter[c]/self.union[c] if self.union[c] > 0 else np.nan)\n",
    "        return np.nanmean(binary_dice_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For test purpose, we create a tensor, `x1`, as a prediction for the first channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 4, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1a = torch.ones(20,1,1,1)\n",
    "x1b = torch.clone(x1a)*0.5\n",
    "x1c = torch.clone(x1a)*0.3\n",
    "x1d = torch.clone(x1a)*0.1\n",
    "\n",
    "x1 = torch.cat((x1a,x1b,x1c,x1d),dim=1)   # Prediction: 20x4\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is a flatten mask, used by fastai segmentation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]],\n",
       " \n",
       "         [[0.]]]),\n",
       " torch.Size([20, 1, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.zeros(20,1,1)  # Target: 20xClass0\n",
    "x2[0:5], x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_obj = DiceMulti()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test of `DiceMulti` into a simulated training with `compute_val` and a test Learner with `TstLearner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice metric = 1\n",
    "test_eq(compute_val(dice_obj, x1, x2), 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = torch.ones(20,1,1)  # Target: 20xClass1\n",
    "# Dice metric = 0\n",
    "test_eq(compute_val(dice_obj, x1, x2), 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different scenario with a multiclass batch:\n",
    "- Class0 x 10\n",
    "- Class1 x 4\n",
    "- Class2 x 3\n",
    "- Class4 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2a = torch.zeros(10,1,1)\n",
    "x2b = torch.ones(4,1,1)\n",
    "x2c = torch.ones(3,1,1) * 2\n",
    "x2d = torch.ones(3,1,1) * 3\n",
    "\n",
    "x2 = torch.cat((x2a,x2b,x2c,x2d),dim=0)   # Target: 10xClass0, 4xClass1, 3xClass2, 3xClass4\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6666666666666666, 0, 0, 0]]\n",
      "0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "dice1 = (2*10)/(2*10+4+3+3)              # Dice: 2*TP/(2*TP+FP+FN)\n",
    "dice2 = 0\n",
    "dice3 = 0\n",
    "dice4 = 0\n",
    "# Value to be tested\n",
    "computed_dice = compute_val(dice_obj, x1, x2)\n",
    "# Dice metric = 0.1666\n",
    "test_eq(computed_dice, (dice1+dice2+dice3+dice4)/4)\n",
    "print(f\"[{[dice1, dice2, dice3, dice4]}]\")\n",
    "print(computed_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 10.0, 1: 0.0, 2: 0.0, 3: 0.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_obj.inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 30.0, 1: 4.0, 2: 3.0, 3: 3.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_obj.union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dice_scores = np.array([])\n",
    "for c in dice_obj.inter:\n",
    "    binary_dice_scores = np.append(\n",
    "        binary_dice_scores, \n",
    "        2.*dice_obj.inter[c]/dice_obj.union[c] \n",
    "        if dice_obj.union[c] > 0 else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66666667, 0.        , 0.        , 0.        ])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_dice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean(binary_dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class KaggleDice(Metric):\n",
    "    def __init__(self, axis=1, eps=1e-9): self.axis, self.eps = axis, eps\n",
    "    def reset(self): self.inter, self.union = {}, {}\n",
    "    \n",
    "    def accumulate(self, learn):\n",
    "        y = learn.yb[0]\n",
    "        preds = learn.pred\n",
    "        \n",
    "        n, c = y.shape[0], preds.shape[self.axis]\n",
    "        \n",
    "        pred = preds.argmax(dim=self.axis).view(n, -1)\n",
    "        targs = y.view(n, -1)\n",
    "        \n",
    "        pred, targ = flatten_check(pred, targs)\n",
    "        for i in range(0, c):\n",
    "            p = torch.where(pred == i, 1, 0)\n",
    "            t = torch.where(targs == i, 1, 0)\n",
    "            c_inter = (p*t).sum(-1).float()#.item()\n",
    "            c_union = (p+t).sum(-1).float()#.item()\n",
    "            if i in self.inter:\n",
    "                self.inter[i] = torch.cat([self.inter[i], c_inter], dim=0)\n",
    "                self.union[i] = torch.cat([self.union[i], c_union], dim=0)\n",
    "            else:\n",
    "                self.inter[i] = c_inter\n",
    "                self.union[i] = c_union\n",
    "    \n",
    "    @property\n",
    "    def value(self):\n",
    "        binary_dice_scores = np.array([])\n",
    "        for c in range(len(self.inter)):\n",
    "            cond = self.union[c] == 0\n",
    "            val = 2.*(self.inter[c]+self.eps)/(self.union[c]+self.eps)\n",
    "            val[cond] = 1\n",
    "            binary_dice_scores = np.append(binary_dice_scores, val)\n",
    "        return np.nanmean(binary_dice_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition \"Evaluation\" metric is defined as:\n",
    "\n",
    "> This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n",
    "\n",
    "$$\n",
    "J(A,B) = \\frac{2 * |A \\cap B|}{|A| \\cup |B|}\n",
    "$$\n",
    "\n",
    "> where X is the predicted set of pixels and Y is the ground truth. The Dice coefficient is defined to be 1 when both X and Y are empty. The leaderboard score is the mean of the Dice coefficients for each <ImageId, ClassId> pair in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_kaggle_obj = KaggleDice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_kaggle_dice = compute_val(dice_kaggle_obj, x1, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7500000000777778"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dice_kaggle_obj.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for `KaggleDice`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [0,6,15,20]\n",
    "intersect, union = {}, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    pred = x1[vals[i]:vals[i+1]].clone()\n",
    "    targs = (x2[vals[i]:vals[i+1]].clone(),)\n",
    "    eps = 1e-9\n",
    "\n",
    "    #pred[0]\n",
    "\n",
    "    n, c = targs[0].shape[0], pred.shape[1]\n",
    "    #n, c\n",
    "\n",
    "    pred = pred.argmax(dim=1).view(n, -1)\n",
    "    targs = targs[0].view(n, -1)\n",
    "    #pred.shape, targs.shape\n",
    "\n",
    "    #pred[0]\n",
    "\n",
    "    for i in range(0, c):\n",
    "        p = torch.where(pred == i, 1, 0)\n",
    "        t = torch.where(targs == i, 1, 0)\n",
    "        #p, t = TensorBase(p), TensorBase(t)\n",
    "        c_inter = (p*t).sum(-1).float()#.item()\n",
    "        c_union = (p+t).sum(-1).float()#.item()\n",
    "        if i in intersect:\n",
    "            intersect[i] = torch.cat([intersect[i], c_inter], dim=0)\n",
    "            union[i] = torch.cat([union[i], c_union], dim=0)\n",
    "        else:\n",
    "            intersect[i] = c_inter\n",
    "            union[i] = c_union\n",
    "\n",
    "        #print(f\"Iter n.{i}\\nintersect: {intersect[i]}\\nunion: {union[i]}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dice_scores = np.array([])\n",
    "for c in intersect.keys():\n",
    "    cond = union[c] == 0\n",
    "    val = 2.*(intersect[c]+eps)/(union[c]+eps)\n",
    "    val[cond] = 1\n",
    "    binary_dice_scores = np.append(binary_dice_scores, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 4\n",
    "fig, axs = plt.subplots(num, 1, sharex=True)\n",
    "for i in range(num):\n",
    "    axs[i].plot(binary_dice_scores[(i*20):((i*20)+20)])\n",
    "    axs[i].set_title(i, loc='right', y=.6, pad=-7.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_close(computed_kaggle_dice, binary_dice_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def dice_kaggle(\n",
    "    preds: Tensor, \n",
    "    targs: Tensor, \n",
    "    iou: bool = False, \n",
    "    eps: float = 1e-8\n",
    "):\n",
    "    \"\"\"\n",
    "    The metric of the competition, \n",
    "    if there's no defect in `targs` and no defects in `preds`: dice=1.\n",
    "    \"\"\"\n",
    "    n, c = targs.shape[0], preds.shape[1]\n",
    "    preds = preds.argmax(dim=1).view(n, -1)\n",
    "    targs = targs.view(n, -1)\n",
    "\n",
    "    intersect_list, union_list = [], []\n",
    "    for i in range(c):\n",
    "        inp, trgs = TensorBase(preds), TensorBase(targs)\n",
    "        \n",
    "        inter = ((inp == i) & (trgs == i)).sum(-1).float()\n",
    "        un = ((inp == i).sum(-1) + (trgs == i).sum(-1))\n",
    "        \n",
    "        intersect_list.append(inter)\n",
    "        union_list.append(un)\n",
    "\n",
    "    intersect = torch.stack(intersect_list)\n",
    "    union = torch.stack(union_list)\n",
    "\n",
    "    if not iou:\n",
    "        dice = ((2.0 * intersect + eps) / (union + eps))\n",
    "        return dice.mean()\n",
    "    else:\n",
    "        int_over_union = ((intersect + eps) / (union - intersect + eps)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice_kag = dice_kaggle(x1, x2)\n",
    "dice_kag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ne(computed_dice, dice_kag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "fastai_dice_metrics = [ModDiceMulti()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intersection Over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def iou(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None,\n",
    "    activation: str = \"Sigmoid\"\n",
    "):\n",
    "    \"\"\"\n",
    "    https://github.com/catalyst-team/catalyst/blob/master/catalyst/dl/utils/criterion/iou.py\n",
    "    Args:\n",
    "        outputs (torch.Tensor): A list of predicted elements\n",
    "        targets (torch.Tensor):  A list of elements that are to be predicted\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold (float): threshold for outputs binarization\n",
    "        activation (str): An torch.nn activation applied to the outputs.\n",
    "            Must be one of [\"none\", \"Sigmoid\", \"Softmax2d\"]\n",
    "    Returns:\n",
    "        float: IoU (Jaccard) score\n",
    "    \"\"\"\n",
    "    outputs = F.sigmoid(outputs)\n",
    "\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "\n",
    "    intersection = torch.sum(targets * outputs)\n",
    "    union = torch.sum(targets) + torch.sum(outputs)\n",
    "    iou = intersection / (union - intersection + eps)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def iou_binary_metric(preds, labels, EMPTY=1., ignore=None, per_image=True):\n",
    "    \"\"\"\n",
    "    IoU for foreground class\n",
    "    binary: 1 foreground, 0 background\n",
    "    \"\"\"\n",
    "    if not per_image:\n",
    "        preds, labels = (preds,), (labels,)\n",
    "    ious = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        intersection = ((label == 1) & (pred == 1)).sum()\n",
    "        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n",
    "        if not union:\n",
    "            iou = EMPTY\n",
    "        else:\n",
    "            iou = float(intersection) / float(union)\n",
    "        ious.append(iou)\n",
    "    iou = mean(ious)    # mean accross images if per_image\n",
    "    return 100 * iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def iou_metric(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n",
    "    \"\"\"\n",
    "    Array of IoU for each (non ignored) class\n",
    "    \"\"\"\n",
    "    if not per_image:\n",
    "        preds, labels = (preds,), (labels,)\n",
    "    ious = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        iou = []    \n",
    "        for i in range(C):\n",
    "            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n",
    "                intersection = ((label == i) & (pred == i)).sum()\n",
    "                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n",
    "                if not union:\n",
    "                    iou.append(EMPTY)\n",
    "                else:\n",
    "                    iou.append(float(intersection) / float(union))\n",
    "        ious.append(iou)\n",
    "    ious = [mean(iou) for iou in zip(*ious)] # mean accross images if per_image\n",
    "    return 100 * np.array(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    \"\"\"computes IoU for one ground truth mask and predicted mask\"\"\"\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = torch.zeros(20,4,1,1)\n",
    "for i in range(4):\n",
    "    x3[:, i, :, :] = torch.where(x2 == i, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    print(compute_ious(x1[:, i].numpy(), x3[:, i].numpy(), classes=[0,1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    \"\"\"computes mean iou for a batch of ground truth masks and predicted masks\"\"\"\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_iou_batch(x1, x3, classes=[0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dice(\n",
    "    outputs: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    eps: float = 1e-7,\n",
    "    threshold: float = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the binary dice metric\n",
    "    Args:\n",
    "        outputs (list):  A list of predicted elements\n",
    "        targets (list): A list of elements that are to be predicted\n",
    "        eps (float): epsilon\n",
    "        threshold (float): threshold for outputs binarization\n",
    "    Returns:\n",
    "        double:  Dice score\n",
    "    \"\"\"\n",
    "    outputs = F.sigmoid(outputs)\n",
    "\n",
    "    if threshold is not None:\n",
    "        outputs = (outputs > threshold).float()\n",
    "\n",
    "    intersection = torch.sum(targets * outputs)\n",
    "    union = torch.sum(targets) + torch.sum(outputs)\n",
    "    dice = 2 * intersection / (union + eps)\n",
    "\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there'are some functions used by the pure pytorch solution from a kaggle kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    \"\"\"\n",
    "    Calculates dice of positive and negative images seperately\n",
    "    `probability` and `truth` must be `torch.Tensors`.\n",
    "    \"\"\"\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric(x1, x3)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def predict(X, threshold):\n",
    "    \"\"\"X is sigmoid output of the model\"\"\"\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Meter:\n",
    "    \"\"\"A meter to keep track of iou and dice scores throughout an epoch\"\"\"\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        \n",
    "        self.base_dice_scores.extend(dice.tolist())\n",
    "        self.dice_pos_scores.extend(dice_pos.tolist())\n",
    "        self.dice_neg_scores.extend(dice_neg.tolist())\n",
    "        \n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        \n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Calc the mean of dices metrics (dice, dice_neg, dice_pos)\n",
    "        and IoU mean.\n",
    "        \n",
    "        Returns: \n",
    "            `dices` as list of means `[dice, dice_neg, dice_pos]`,\n",
    "            `iou` as mean of IoUs\n",
    "        \"\"\"\n",
    "        dice     = np.nanmean(self.base_dice_scores)\n",
    "        dice_neg = np.nanmean(self.dice_neg_scores)\n",
    "        dice_pos = np.nanmean(self.dice_pos_scores)\n",
    "        \n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        \n",
    "        return dices, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(Meter.get_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    \"\"\"logging the metrics at the end of an epoch\"\"\"\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(f\"Loss: {epoch_loss:.4f} | IoU: {iou:.4f} | dice: {dice:.4f} | dice_neg: {dice_neg:.4f} | dice_pos: {dice_pos:.4f}\")\n",
    "    return dice, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
