{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# default_exp optimizer\r\n",
    "# all_slow"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizer\r\n",
    "> Utility functions for FastAI optimizers in Pytorch models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some utility functions for training Pytorch models with FastAI fine tuning method. The code is from [this repository](https://github.com/muellerzr/fastai_minima/blob/master/fastai_minima/optimizer.py#L159)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# export\r\n",
    "import torch.nn as nn\r\n",
    "from fastcore.foundation import L\r\n",
    "from fastai.optimizer import OptimWrapper"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# export\r\n",
    "def params(m):\r\n",
    "    \"Return all parameters of `m` (Pytorch model).\"\r\n",
    "    return [p for p in m.parameters()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# export\r\n",
    "def convert_params(o:list) -> list:\r\n",
    "    \"\"\"\r\n",
    "    Converts `o` into Pytorch-compatable param groups\r\n",
    "    `o` should be a set of layer-groups that should be split in the optimizer\r\n",
    "    Example:\r\n",
    "    ```python\r\n",
    "    def splitter(m): return convert_params([[m.a], [m.b]])\r\n",
    "    ```\r\n",
    "    Where `m` is a model defined as:\r\n",
    "    ```python\r\n",
    "    class RegModel(Module):\r\n",
    "      def __init__(self): self.a,self.b = nn.Parameter(torch.randn(1)),nn.Parameter(torch.randn(1))\r\n",
    "      def forward(self, x): return x*self.a + self.b\r\n",
    "    ```\r\n",
    "    \"\"\"\r\n",
    "    if not isinstance(o[0], dict):\r\n",
    "        splitter = []\r\n",
    "        for group in o:\r\n",
    "            if not isinstance(group[0], nn.parameter.Parameter):\r\n",
    "                group = L(group).map(params)[0]\r\n",
    "            splitter.append({'params':group})\r\n",
    "        return splitter\r\n",
    "    return o"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the FastAI library is largely used transfer learning with layer-group learning rate freezing. \r\n",
    "The `convert_params` function returns a list of parameters for specific layers in a model that allows discriminative learning rates."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#export\r\n",
    "def smp_splitter(model):\r\n",
    "    return convert_params([[model.encoder],[model.decoder],[model.segmentation_head]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#export\r\n",
    "def opt_func(params, torch_opt, *args, **kwargs):\r\n",
    "    \"\"\"Pytorch Optimizer for fastai Learner.\"\"\"\r\n",
    "    return OptimWrapper(params, torch_opt, *args, **kwargs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('steel_segmentation': conda)"
  },
  "interpreter": {
   "hash": "28655ac308a3066e78dcd35793da650bd1be67b358d93812a1ea5ca876afb9ba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}