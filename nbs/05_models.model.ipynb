{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "> Deep Learning modules with Fastai/Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from steel_segmentation.core import *\n",
    "from steel_segmentation.data import *\n",
    "from steel_segmentation.preprocessing import *\n",
    "from steel_segmentation.models.dls import *\n",
    "from steel_segmentation.models.metrics import *\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    from fastai.vision.all import *\n",
    "    import fastai\n",
    "from fastcore.foundation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "only_imgs = [\"0a1cade03.jpg\", \"bca4ae758.jpg\", \"988cf521f.jpg\", \"b6a257b28.jpg\",\n",
    "             \"b2ad335bf.jpg\", \"72aaba8ad.jpg\", \"f383950e8.jpg\"]\n",
    "train = train[train[\"ImageId\"].isin(only_imgs)].copy()\n",
    "train_all = train_all[train_all[\"ImageId\"].isin(only_imgs)].copy()\n",
    "train_multi = train_multi[train_multi[\"ImageId\"].isin(only_imgs)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a classification model to get an encoder that know how to classify defects pixels.\n",
    "Then, we build a UNet from the trained encoder and train a segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "models_dir = path.parent / \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Paperspace Gradient machine I stored these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../models/ResNet18-Unet-256-stage1.pth\n",
      "../models/ResNet18-Unet-256-stage2.pth\n",
      "../models/ResNet18_kaggle_class.pth\n",
      "../models/ResNet18-Unet-256-stage3.pth\n",
      "../models/.ipynb_checkpoints\n",
      "../models/kaggle_model.pth\n",
      "../models/ResNet34-Unet-128-stage3.pth\n",
      "../models/ResNet34-Unet-128-stage2.pth\n"
     ]
    }
   ],
   "source": [
    "# missing\n",
    "print_competition_data(models_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "class_metrics = [accuracy_multi, PrecisionMulti(), RecallMulti()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code allow me to create a classification `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing\n",
    "bs = 4\n",
    "\n",
    "dls = get_classification_dls(bs)\n",
    "arch = partial(resnet18, pretrained=True)\n",
    "class_learner = cnn_learner(dls=dls, arch=arch, metrics=class_metrics, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exports\n",
    "seg_metrics = [DiceMulti(), dice_kaggle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines of code allow me to create a segmentation `Learner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing\n",
    "bs = 4 \n",
    "szs = (128, 800)\n",
    "\n",
    "dls = get_segmentation_dls_from_df(train_multi, bs, szs)\n",
    "segmentation_learner = unet_learner(\n",
    "        dls=dls, arch=resnet18, metrics=seg_metrics, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load a custom head in a Unet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing\n",
    "encoder_path = models_dir / \"ResNet18-2_class.pt\"\n",
    "segmentation_learner.model[0].load_state_dict(\n",
    "            torch.load(encoder_path), strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this project is not only FastAi. I based an alternative solution based on this [kernel](https://www.kaggle.com/rishabhiitbhu/unet-starter-kernel-pytorch-lb-0-88). \n",
    "In this notebook I will go through each part of the model from that kernel.\n",
    "\n",
    "Starting from the Unet architecture from the [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import functools\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision.models.resnet import ResNet\n",
    "from torchvision.models.resnet import BasicBlock\n",
    "from torchvision.models.resnet import Bottleneck\n",
    "from pretrainedmodels.models.torchvision_models import pretrained_settings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def preprocess_input(x, mean=None, std=None, input_space='RGB', input_range=None, **kwargs):\n",
    "    \"\"\"Preprocessing the `x` inputs with normalization.\"\"\"\n",
    "    if input_space == 'BGR':\n",
    "        x = x[..., ::-1].copy()\n",
    "\n",
    "    if input_range is not None:\n",
    "        if x.max() > 1 and input_range[1] == 1:\n",
    "            x = x / 255.\n",
    "\n",
    "    if mean is not None:\n",
    "        mean = np.array(mean)\n",
    "        x = x - mean\n",
    "\n",
    "    if std is not None:\n",
    "        std = np.array(std)\n",
    "        x = x / std\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Model(nn.Module):\n",
    "    \"\"\"Custom `nn.Module` class with `kaiming_normal_` initialization\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Conv2dReLU(nn.Module):\n",
    "    \"\"\"Conv-(BatchNorm)-Relu block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=0,\n",
    "                 stride=1, use_batchnorm=True, **batchnorm_params):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              stride=stride, padding=padding, bias=not (use_batchnorm)),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        if use_batchnorm:\n",
    "            layers.insert(1, nn.BatchNorm2d(out_channels, **batchnorm_params))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EncoderDecoder(Model):\n",
    "\n",
    "    def __init__(self, encoder, decoder, activation):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        if callable(activation) or activation is None:\n",
    "            self.activation = activation\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            raise ValueError('Activation should be \"sigmoid\"/\"softmax\"/callable/None')\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Sequentially pass `x` trough model`s `encoder` and `decoder` (return logits!)\"\"\"\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Inference method. Switch model to `eval` mode, call `.forward(x)`\n",
    "        and apply activation function (if activation is not `None`) with `torch.no_grad()`\n",
    "\n",
    "        Args:\n",
    "            x: 4D torch tensor with shape (batch_size, channels, height, width)\n",
    "\n",
    "        Return:\n",
    "            prediction: 4D torch tensor with shape (batch_size, classes, height, width)\n",
    "\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.forward(x)\n",
    "            if self.activation:\n",
    "                x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_batchnorm=True):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            Conv2dReLU(in_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n",
    "            Conv2dReLU(out_channels, out_channels, kernel_size=3, padding=1, use_batchnorm=use_batchnorm),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, skip = x\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CenterBlock(DecoderBlock):\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class UnetDecoder(Model):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            final_channels=1,\n",
    "            use_batchnorm=True,\n",
    "            center=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if center:\n",
    "            channels = encoder_channels[0]\n",
    "            self.center = CenterBlock(channels, channels, use_batchnorm=use_batchnorm)\n",
    "        else:\n",
    "            self.center = None\n",
    "\n",
    "        in_channels = self.compute_channels(encoder_channels, decoder_channels)\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.layer1 = DecoderBlock(in_channels[0], out_channels[0], use_batchnorm=use_batchnorm)\n",
    "        self.layer2 = DecoderBlock(in_channels[1], out_channels[1], use_batchnorm=use_batchnorm)\n",
    "        self.layer3 = DecoderBlock(in_channels[2], out_channels[2], use_batchnorm=use_batchnorm)\n",
    "        self.layer4 = DecoderBlock(in_channels[3], out_channels[3], use_batchnorm=use_batchnorm)\n",
    "        self.layer5 = DecoderBlock(in_channels[4], out_channels[4], use_batchnorm=use_batchnorm)\n",
    "        self.final_conv = nn.Conv2d(out_channels[4], final_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self.initialize()\n",
    "\n",
    "    def compute_channels(self, encoder_channels, decoder_channels):\n",
    "        channels = [\n",
    "            encoder_channels[0] + encoder_channels[1],\n",
    "            encoder_channels[2] + decoder_channels[0],\n",
    "            encoder_channels[3] + decoder_channels[1],\n",
    "            encoder_channels[4] + decoder_channels[2],\n",
    "            0 + decoder_channels[3],\n",
    "        ]\n",
    "        return channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoder_head = x[0]\n",
    "        skips = x[1:]\n",
    "\n",
    "        if self.center:\n",
    "            encoder_head = self.center(encoder_head)\n",
    "\n",
    "        x = self.layer1([encoder_head, skips[0]])\n",
    "        x = self.layer2([x, skips[1]])\n",
    "        x = self.layer3([x, skips[2]])\n",
    "        x = self.layer4([x, skips[3]])\n",
    "        x = self.layer5([x, None])\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ResNetEncoder(ResNet):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pretrained = False\n",
    "        del self.fc\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.conv1(x)\n",
    "        x0 = self.bn1(x0)\n",
    "        x0 = self.relu(x0)\n",
    "\n",
    "        x1 = self.maxpool(x0)\n",
    "        x1 = self.layer1(x1)\n",
    "\n",
    "        x2 = self.layer2(x1)\n",
    "        x3 = self.layer3(x2)\n",
    "        x4 = self.layer4(x3)\n",
    "\n",
    "        return [x4, x3, x2, x1, x0]\n",
    "\n",
    "    def load_state_dict(self, state_dict, **kwargs):\n",
    "        state_dict.pop('fc.bias')\n",
    "        state_dict.pop('fc.weight')\n",
    "        super().load_state_dict(state_dict, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "resnet_encoders = {\n",
    "    'resnet18': {\n",
    "        'encoder': ResNetEncoder,\n",
    "        'pretrained_settings': pretrained_settings['resnet18'],\n",
    "        'out_shapes': (512, 256, 128, 64, 64),\n",
    "        'params': {\n",
    "            'block': BasicBlock,\n",
    "            'layers': [2, 2, 2, 2],\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'resnet34': {\n",
    "        'encoder': ResNetEncoder,\n",
    "        'pretrained_settings': pretrained_settings['resnet34'],\n",
    "        'out_shapes': (512, 256, 128, 64, 64),\n",
    "        'params': {\n",
    "            'block': BasicBlock,\n",
    "            'layers': [3, 4, 6, 3],\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'resnet50': {\n",
    "        'encoder': ResNetEncoder,\n",
    "        'pretrained_settings': pretrained_settings['resnet50'],\n",
    "        'out_shapes': (2048, 1024, 512, 256, 64),\n",
    "        'params': {\n",
    "            'block': Bottleneck,\n",
    "            'layers': [3, 4, 6, 3],\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'resnet101': {\n",
    "        'encoder': ResNetEncoder,\n",
    "        'pretrained_settings': pretrained_settings['resnet101'],\n",
    "        'out_shapes': (2048, 1024, 512, 256, 64),\n",
    "        'params': {\n",
    "            'block': Bottleneck,\n",
    "            'layers': [3, 4, 23, 3],\n",
    "        },\n",
    "    },\n",
    "\n",
    "    'resnet152': {\n",
    "        'encoder': ResNetEncoder,\n",
    "        'pretrained_settings': pretrained_settings['resnet152'],\n",
    "        'out_shapes': (2048, 1024, 512, 256, 64),\n",
    "        'params': {\n",
    "            'block': Bottleneck,\n",
    "            'layers': [3, 8, 36, 3],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "encoders = {}\n",
    "encoders.update(resnet_encoders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_encoder(name, encoder_weights=None):\n",
    "    Encoder = encoders[name]['encoder']\n",
    "    encoder = Encoder(**encoders[name]['params'])\n",
    "    encoder.out_shapes = encoders[name]['out_shapes']\n",
    "\n",
    "    if encoder_weights is not None:\n",
    "        settings = encoders[name]['pretrained_settings'][encoder_weights]\n",
    "        encoder.load_state_dict(model_zoo.load_url(settings['url']))\n",
    "\n",
    "    return encoder\n",
    "\n",
    "def get_encoder_names():\n",
    "    return list(encoders.keys())\n",
    "\n",
    "def get_preprocessing_fn(encoder_name, pretrained='imagenet'):\n",
    "    settings = encoders[encoder_name]['pretrained_settings']\n",
    "\n",
    "    if pretrained not in settings.keys():\n",
    "        raise ValueError('Avaliable pretrained options {}'.format(settings.keys()))\n",
    "\n",
    "    input_space = settings[pretrained].get('input_space')\n",
    "    input_range = settings[pretrained].get('input_range')\n",
    "    mean = settings[pretrained].get('mean')\n",
    "    std = settings[pretrained].get('std')\n",
    "    \n",
    "    return functools.partial(preprocess_input, mean=mean, std=std, input_space=input_space, input_range=input_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Unet(EncoderDecoder):\n",
    "    \"\"\"Unet_ is a fully convolution neural network for image semantic segmentation\n",
    "\n",
    "    Args:\n",
    "        encoder_name: name of classification model (without last dense layers) used as feature\n",
    "            extractor to build segmentation model.\n",
    "        encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "        decoder_channels: list of numbers of ``Conv2D`` layer filters in decoder blocks\n",
    "        decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n",
    "            is used.\n",
    "        classes: a number of classes for output (output shape - ``(batch, classes, h, w)``).\n",
    "        activation: activation function used in ``.predict(x)`` method for inference.\n",
    "            One of [``sigmoid``, ``softmax``, callable, None]\n",
    "        center: if ``True`` add ``Conv2dReLU`` block on encoder head (useful for VGG models)\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **Unet**\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/pdf/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_name='resnet34',\n",
    "            encoder_weights='imagenet',\n",
    "            decoder_use_batchnorm=True,\n",
    "            decoder_channels=(256, 128, 64, 32, 16),\n",
    "            classes=1,\n",
    "            activation='sigmoid',\n",
    "            center=False,  # usefull for VGG models\n",
    "    ):\n",
    "        encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            encoder_weights=encoder_weights\n",
    "        )\n",
    "\n",
    "        decoder = UnetDecoder(\n",
    "            encoder_channels=encoder.out_shapes,\n",
    "            decoder_channels=decoder_channels,\n",
    "            final_channels=classes,\n",
    "            use_batchnorm=decoder_use_batchnorm,\n",
    "            center=center,\n",
    "        )\n",
    "\n",
    "        super().__init__(encoder, decoder, activation)\n",
    "\n",
    "        self.name = 'u-{}'.format(encoder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_data.ipynb.\n",
      "Converted 02_preprocessing.ipynb.\n",
      "Converted 03_models.dls.ipynb.\n",
      "Converted 04_model.metrics.ipynb.\n",
      "Converted 05_models.module.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
