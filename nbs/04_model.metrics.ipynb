{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> A collection of Metrics used in the segmentation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from steel_segmentation.core import *\n",
    "from steel_segmentation.data import *\n",
    "from steel_segmentation.preprocessing import *\n",
    "from steel_segmentation.models.dls import *\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    from fastai.vision.all import *\n",
    "    import fastai\n",
    "from fastcore.foundation import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class DiceMulti(Metric):\n",
    "    \"Averaged Dice metric (Macro F1) for multiclass target in segmentation\"\n",
    "\n",
    "    def __init__(self, axis=1): self.axis = axis\n",
    "    def reset(self): self.inter, self.union = {}, {}\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        pred, targ = flatten_check(learn.pred.argmax(dim=self.axis), learn.y)\n",
    "        for c in range(learn.pred.shape[self.axis]):\n",
    "            p = torch.where(pred == c, 1, 0)\n",
    "            t = torch.where(targ == c, 1, 0)\n",
    "            p, t = TensorBase(p), TensorBase(t)\n",
    "            c_inter = (p*t).float().sum().item()\n",
    "            c_union = (p+t).float().sum().item()\n",
    "            if c in self.inter:\n",
    "                self.inter[c] += c_inter\n",
    "                self.union[c] += c_union\n",
    "            else:\n",
    "                self.inter[c] = c_inter\n",
    "                self.union[c] = c_union\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        binary_dice_scores = np.array([])\n",
    "        for c in self.inter:\n",
    "            binary_dice_scores = np.append(\n",
    "                binary_dice_scores, 2.*self.inter[c]/self.union[c] if self.union[c] > 0 else np.nan)\n",
    "        return np.nanmean(binary_dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def dice_kaggle(input: Tensor, targs: Tensor, iou: bool = False, eps: float = 1e-8):\n",
    "    \"\"\"From [kaggle](https://www.kaggle.com/iafoss/severstal-fast-ai-256x256-crops)\"\"\"\n",
    "    n, c = targs.shape[0], input.shape[1]\n",
    "    input = input.argmax(dim=1).view(n, -1)\n",
    "    targs = targs.view(n, -1)\n",
    "\n",
    "    intersect, union = [], []\n",
    "    for i in range(1, c):\n",
    "        inp, trgs = TensorBase(input), TensorBase(targs)\n",
    "        intersect.append(((inp == i) & (trgs == i)).sum(-1).float())\n",
    "        union.append(((inp == i).sum(-1) + (trgs == i).sum(-1)).float())\n",
    "\n",
    "    intersect = torch.stack(intersect)\n",
    "    union = torch.stack(union)\n",
    "\n",
    "    if not iou:\n",
    "        return ((2.0*intersect + eps) / (union+eps)).mean()\n",
    "    else:\n",
    "        return ((intersect + eps) / (union - intersect + eps)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "def dice_forum(input: Tensor, targs: Tensor, eps: float = 1e-8):\n",
    "    \"\"\"\n",
    "    A not valuable metric from\n",
    "    [fastai forum](https://forums.fast.ai/t/training-unet-for-segmentation-negative-dice-score/45137/9)\n",
    "    \"\"\"\n",
    "    n = targs.shape[0]  # channels\n",
    "    targs = targs.squeeze(1)  # add batch dimension\n",
    "\n",
    "    input = input.argmax(dim=1).view(n, -1)\n",
    "\n",
    "    targs = targs.view(n, -1)\n",
    "    targs1 = (targs > 0).float()\n",
    "    input1 = (input > 0).float()\n",
    "\n",
    "    ss = (input == targs).float()\n",
    "    intersect = (ss * targs1).sum(dim=1).float()\n",
    "    union = (input1+targs1).sum(dim=1).float()\n",
    "\n",
    "    l = 2. * intersect / union\n",
    "    l[union == 0.] = 1.\n",
    "\n",
    "    return l.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n",
    "    \"\"\"computes IoU for one ground truth mask and predicted mask\"\"\"\n",
    "    pred[label == ignore_index] = 0\n",
    "    ious = []\n",
    "    for c in classes:\n",
    "        label_c = label == c\n",
    "        if only_present and np.sum(label_c) == 0:\n",
    "            ious.append(np.nan)\n",
    "            continue\n",
    "        pred_c = pred == c\n",
    "        intersection = np.logical_and(pred_c, label_c).sum()\n",
    "        union = np.logical_or(pred_c, label_c).sum()\n",
    "        if union != 0:\n",
    "            ious.append(intersection / union)\n",
    "    return ious if ious else [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def compute_iou_batch(outputs, labels, classes=None):\n",
    "    \"\"\"computes mean iou for a batch of ground truth masks and predicted masks\"\"\"\n",
    "    ious = []\n",
    "    preds = np.copy(outputs) # copy is imp\n",
    "    labels = np.array(labels) # tensor to np\n",
    "    for pred, label in zip(preds, labels):\n",
    "        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n",
    "    iou = np.nanmean(ious)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def metric(probability, truth, threshold=0.5, reduction='none'):\n",
    "    \"\"\"\n",
    "    Calculates dice of positive and negative images seperately\n",
    "    `probability` and `truth` must be `torch.Tensors`.\n",
    "    \"\"\"\n",
    "    batch_size = len(truth)\n",
    "    with torch.no_grad():\n",
    "        probability = probability.view(batch_size, -1)\n",
    "        truth = truth.view(batch_size, -1)\n",
    "        assert(probability.shape == truth.shape)\n",
    "\n",
    "        p = (probability > threshold).float()\n",
    "        t = (truth > 0.5).float()\n",
    "\n",
    "        t_sum = t.sum(-1)\n",
    "        p_sum = p.sum(-1)\n",
    "        neg_index = torch.nonzero(t_sum == 0)\n",
    "        pos_index = torch.nonzero(t_sum >= 1)\n",
    "\n",
    "        dice_neg = (p_sum == 0).float()\n",
    "        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n",
    "\n",
    "        dice_neg = dice_neg[neg_index]\n",
    "        dice_pos = dice_pos[pos_index]\n",
    "        dice = torch.cat([dice_pos, dice_neg])\n",
    "\n",
    "        num_neg = len(neg_index)\n",
    "        num_pos = len(pos_index)\n",
    "\n",
    "    return dice, dice_neg, dice_pos, num_neg, num_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def predict(X, threshold):\n",
    "    \"\"\"X is sigmoid output of the model\"\"\"\n",
    "    X_p = np.copy(X)\n",
    "    preds = (X_p > threshold).astype('uint8')\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Meter:\n",
    "    \"\"\"A meter to keep track of iou and dice scores throughout an epoch\"\"\"\n",
    "    def __init__(self, phase, epoch):\n",
    "        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n",
    "        self.base_dice_scores = []\n",
    "        self.dice_neg_scores = []\n",
    "        self.dice_pos_scores = []\n",
    "        self.iou_scores = []\n",
    "\n",
    "    def update(self, targets, outputs):\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n",
    "        \n",
    "        self.base_dice_scores.extend(dice.tolist())\n",
    "        self.dice_pos_scores.extend(dice_pos.tolist())\n",
    "        self.dice_neg_scores.extend(dice_neg.tolist())\n",
    "        \n",
    "        preds = predict(probs, self.base_threshold)\n",
    "        \n",
    "        iou = compute_iou_batch(preds, targets, classes=[1])\n",
    "        self.iou_scores.append(iou)\n",
    "\n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Calc the mean of dices metrics (dice, dice_neg, dice_pos)\n",
    "        and IoU mean.\n",
    "        \n",
    "        Returns: \n",
    "            `dices` as list of means `[dice, dice_neg, dice_pos]`,\n",
    "            `iou` as mean of IoUs\n",
    "        \"\"\"\n",
    "        dice     = np.nanmean(self.base_dice_scores)\n",
    "        dice_neg = np.nanmean(self.dice_neg_scores)\n",
    "        dice_pos = np.nanmean(self.dice_pos_scores)\n",
    "        \n",
    "        dices = [dice, dice_neg, dice_pos]\n",
    "        iou = np.nanmean(self.iou_scores)\n",
    "        \n",
    "        return dices, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Meter.get_metrics\" class=\"doc_header\"><code>Meter.get_metrics</code><a href=\"__main__.py#L24\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Meter.get_metrics</code>()\n",
       "\n",
       "Calc the mean of dices metrics (dice, dice_neg, dice_pos)\n",
       "and IoU mean.\n",
       "\n",
       "Returns: \n",
       "    `dices` as list of means `[dice, dice_neg, dice_pos]`,\n",
       "    `iou` as mean of IoUs"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Meter.get_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def epoch_log(phase, epoch, epoch_loss, meter, start):\n",
    "    \"\"\"logging the metrics at the end of an epoch\"\"\"\n",
    "    dices, iou = meter.get_metrics()\n",
    "    dice, dice_neg, dice_pos = dices\n",
    "    print(f\"Loss: {epoch_loss:.4f} | IoU: {iou:.4f} | dice: {dice:.4f} | dice_neg: {dice_neg:.4f} | dice_pos: {dice_pos:.4f}\")\n",
    "    return dice, iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
