{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# missing\n",
    "!git clone https://github.com/marcomatteo/steel_segmentation.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# missing\n",
    "!pip install -e steel_segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "> Various loss functions in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/marcomatteo/steel_segmentation/blob/master/dev_nbs/06_loss.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from steel_segmentation.metadata import *\n",
    "from steel_segmentation.masks import *\n",
    "from steel_segmentation.datasets import *\n",
    "from steel_segmentation.dataloaders import *\n",
    "from steel_segmentation.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module there are various loss functions for binary and instance segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from fastai.torch_core import TensorBase\n",
    "def _contiguous(x): return TensorBase(x.transpose(-1,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lovasz Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\"\"\"\n",
    "Lovasz-Softmax and Jaccard hinge loss in PyTorch\n",
    "Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "try:\n",
    "    from itertools import  ifilterfalse\n",
    "except ImportError: # py3k\n",
    "    from itertools import  filterfalse as ifilterfalse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lovasz-Softmax and Jaccard hinge loss in PyTorch - Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License) from this [repository](https://github.com/bermanmaxim/LovaszSoftmax)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def flatten_probas(probas, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch\n",
    "    \"\"\"\n",
    "    if probas.dim() == 3:\n",
    "        # assumes output of a sigmoid layer\n",
    "        B, H, W = probas.size()\n",
    "        probas = probas.view(B, 1, H, W)\n",
    "    B, C, H, W = probas.size()\n",
    "    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = (labels != ignore)\n",
    "    vprobas = probas[valid.nonzero().squeeze()]\n",
    "    vlabels = labels[valid]\n",
    "    return vprobas, vlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def isnan(x):\n",
    "    \"\"\"Check if x != x, return False if NaN.\"\"\"\n",
    "    return x != x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss:\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * Variable(signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lovasz_softmax(probas, labels, classes='present', per_image=False, ignore=None):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1).\n",
    "              Interpreted as binary (sigmoid) output with outputs of size [B, H, W].\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), classes=classes)\n",
    "                          for prob, lab in zip(probas, labels))\n",
    "    else:\n",
    "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), classes=classes)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lovasz_softmax_flat(probas, labels, classes='present'):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n",
    "    \"\"\"\n",
    "    if probas.numel() == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return probas * 0.\n",
    "    C = probas.size(1)\n",
    "    losses = []\n",
    "    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n",
    "    for c in class_to_sum:\n",
    "        fg = (labels == c).float() # foreground for class c\n",
    "        if (classes == 'present' and fg.sum() == 0):\n",
    "            continue\n",
    "        if C == 1:\n",
    "            if len(classes) > 1:\n",
    "                raise ValueError('Sigmoid output possible only with 1 class')\n",
    "            class_pred = probas[:, 0]\n",
    "        else:\n",
    "            class_pred = probas[:, c]\n",
    "        errors = (Variable(fg) - class_pred).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "        perm = perm.data\n",
    "        fg_sorted = fg[perm]\n",
    "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
    "    return mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BCE and SoftDice loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\"\"\"\n",
    "https://github.com/asanakoy/kaggle_carvana_segmentation/blob/master/asanakoy/losses.py\n",
    "https://github.com/catalyst-team/catalyst/blob/master/catalyst/dl/utils/criterion/dice.py\n",
    "\"\"\"\n",
    "from functools import partial\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import _Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section there are some loss functions used by @khornlund in his [repository](https://github.com/khornlund/severstal-steel-defect-detection) for the Severstal competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(output, target):\n",
    "    \"\"\"BCE with logits from Pytorch.\"\"\"\n",
    "    return F.binary_cross_entropy_with_logits(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SoftDiceLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        probs = F.sigmoid(logits)\n",
    "        num = labels.size(0)\n",
    "        m1 = probs.view(num, -1)\n",
    "        m2 = labels.view(num, -1)\n",
    "        intersection = (m1 * m2)\n",
    "        score = 2. * (intersection.sum(1) + 1) / (m1.sum(1) + m2.sum(1) + 1)\n",
    "        score = 1 - score.sum() / num\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps: float = 1e-7, threshold: float = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss_fn = partial(\n",
    "            dice,\n",
    "            eps=eps,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        dice = self.loss_fn(logits, targets)\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            eps: float = 1e-7,\n",
    "            threshold: float = None,\n",
    "            bce_weight: float = 0.5,\n",
    "            dice_weight: float = 0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if bce_weight == 0 and dice_weight == 0:\n",
    "            raise ValueError(\n",
    "                \"Both bce_wight and dice_weight cannot be \"\n",
    "                \"equal to 0 at the same time.\"\n",
    "            )\n",
    "\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "        if self.bce_weight != 0:\n",
    "            self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        if self.dice_weight != 0:\n",
    "            self.dice_loss = DiceLoss(eps=eps, threshold=threshold)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        if self.bce_weight == 0:\n",
    "            return self.dice_weight * self.dice_loss(outputs, targets)\n",
    "        if self.dice_weight == 0:\n",
    "            return self.bce_weight * self.bce_loss(outputs, targets)\n",
    "\n",
    "        bce = self.bce_loss(outputs, targets)\n",
    "        dice = self.dice_loss(outputs, targets)\n",
    "        loss = self.bce_weight * bce + self.dice_weight * dice\n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'bce': bce,\n",
    "            'dice': dice\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class IoULoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Intersection over union (Jaccard) loss\n",
    "    Args:\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold (float): threshold for outputs binarization\n",
    "        activation (str): An torch.nn activation applied to the outputs.\n",
    "            Must be one of ['none', 'Sigmoid', 'Softmax2d']\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eps: float = 1e-7,\n",
    "        threshold: float = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.metric_fn = partial(iou, eps=eps, threshold=threshold)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        iou = self.metric_fn(outputs, targets)\n",
    "        return 1 - iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BinaryFocalLoss(_Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        alpha=0.5,\n",
    "        gamma=2,\n",
    "        ignore_index=None,\n",
    "        reduction=\"mean\",\n",
    "        reduced=False,\n",
    "        threshold=0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param alpha:\n",
    "        :param gamma:\n",
    "        :param ignore_index:\n",
    "        :param reduced:\n",
    "        :param threshold:\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        if reduced:\n",
    "            self.focal_loss = partial(\n",
    "                focal_loss_with_logits,\n",
    "                alpha=None,\n",
    "                gamma=gamma,\n",
    "                threshold=threshold,\n",
    "                reduction=reduction,\n",
    "            )\n",
    "        else:\n",
    "            self.focal_loss = partial(\n",
    "                focal_loss_with_logits, alpha=alpha, gamma=gamma, reduction=reduction\n",
    "            )\n",
    "\n",
    "    def forward(self, label_input, label_target):\n",
    "        \"\"\"Compute focal loss for binary classification problem.\n",
    "        \"\"\"\n",
    "        label_target = label_target.view(-1)\n",
    "        label_input = label_input.view(-1)\n",
    "\n",
    "        if self.ignore_index is not None:\n",
    "            # Filter predictions with ignore label from loss computation\n",
    "            not_ignored = label_target != self.ignore_index\n",
    "            label_input = label_input[not_ignored]\n",
    "            label_target = label_target[not_ignored]\n",
    "\n",
    "        loss = self.focal_loss(label_input, label_target)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class FocalBCEDiceLoss(BCEDiceLoss):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            alpha=0.5,\n",
    "            gamma=2,\n",
    "            ignore_index=None,\n",
    "            reduction=\"mean\",\n",
    "            reduced=False,\n",
    "            eps: float = 1e-7,\n",
    "            threshold: float = None,\n",
    "            bce_weight: float = 0.5,\n",
    "            dice_weight: float = 0.5,\n",
    "    ):\n",
    "        super().__init__(eps, threshold, bce_weight, dice_weight)\n",
    "        self.bce_loss = BinaryFocalLoss(alpha, gamma, ignore_index, reduction, reduced, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class LabelSmoother:\n",
    "    \"\"\"\n",
    "    Maps binary labels (0, 1) to (eps, 1 - eps)\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=1e-8):\n",
    "        self.eps = eps\n",
    "        self.scale = 1 - 2 * self.eps\n",
    "        self.bias = self.eps / self.scale\n",
    "\n",
    "    def __call__(self, t):\n",
    "        return (t + self.bias) * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def focal_loss_with_logits(\n",
    "    input: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    gamma=2.0,\n",
    "    alpha: float = 0.25,\n",
    "    reduction=\"mean\",\n",
    "    normalized=False,\n",
    "    threshold: float = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    https://github.com/BloodAxe/pytorch-toolbelt/blob/develop/pytorch_toolbelt/losses/functional.py\n",
    "    Compute binary focal loss between target and output logits.\n",
    "    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.\n",
    "    Args:\n",
    "        input: Tensor of arbitrary shape\n",
    "        target: Tensor of the same shape as input\n",
    "        reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "            'none' | 'mean' | 'sum' | 'batchwise_mean'. 'none': no reduction will be applied,\n",
    "            'mean': the sum of the output will be divided by the number of\n",
    "            elements in the output, 'sum': the output will be summed. Note: :attr:`size_average`\n",
    "            and :attr:`reduce` are in the process of being deprecated, and in the meantime,\n",
    "            specifying either of those two args will override :attr:`reduction`.\n",
    "            'batchwise_mean' computes mean loss per sample in batch. Default: 'mean'\n",
    "        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).\n",
    "        threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).\n",
    "    References::\n",
    "        https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/loss/losses.py\n",
    "    \"\"\"\n",
    "    target = target.type(input.type())\n",
    "\n",
    "    logpt = -F.binary_cross_entropy_with_logits(input, target, reduction=\"none\")\n",
    "    pt = torch.exp(logpt)\n",
    "\n",
    "    # compute the loss\n",
    "    if threshold is None:\n",
    "        focal_term = (1 - pt).pow(gamma)\n",
    "    else:\n",
    "        focal_term = ((1.0 - pt) / threshold).pow(gamma)\n",
    "        focal_term[pt < threshold] = 1\n",
    "\n",
    "    loss = -focal_term * logpt\n",
    "\n",
    "    if alpha is not None:\n",
    "        loss = loss * (alpha * target + (1 - alpha) * (1 - target))\n",
    "\n",
    "    if normalized:\n",
    "        norm_factor = focal_term.sum()\n",
    "        loss = loss / norm_factor\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    if reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "    if reduction == \"batchwise_mean\":\n",
    "        loss = loss.sum(0)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_metadata.ipynb.\n",
      "Converted 02_masks.ipynb.\n",
      "Converted 03_datasets.ipynb.\n",
      "Converted 04_dataloaders.ipynb.\n",
      "Converted 05_metrics.ipynb.\n",
      "Converted 06_trainer.ipynb.\n",
      "Converted 07_predict.ipynb.\n",
      "Converted 08_loss.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
